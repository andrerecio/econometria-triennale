---
title: "Esercitazione N.2"
author: "Econometria"
format: 
  html:
    toc: true
    code-fold: false
    embed-resources: true
execute:
  warning: false
  message: false
---

## Pacchetti necessari

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(knitr)
library(kableExtra)
library(modelsummary)
```

### Per i dati
```{r}
#| warning: false
library(wooldridge)
```

```{r}
data("wage1", package = "wooldridge")
```

## Regressione Multipla

La regressione multipla consente esplicitamente di controllare per altre variabili che influenzano la variabile dipendente e sono potenzialmente correlate con il regressore.

**Idea**: Inserendo le variabili rilevanti nel modello -e quindi togliendole dall’errore- rendiamo più plausibile l’assunzione che le variabile rimaste nel’errore non siano correlate con le variabili del modello.

Nella precedente [Esercitazione](https://andrerecio.github.io/econometria-triennale/Esercitazione1.html), abbiamo stimato il seguente modello di regressione semplice:

$$
wage_i = \beta_0 + \beta_1 educ_i + u_i
$$

Dove $wage$ è misurato in dollari per ora (del 1976) and $educ$ sono gli anni di istruzione.

L'assunzione chiave era: se $E(u_i|educ_i)=0$ allora ha un'interpretazione causale.

Ora stimiamo altri due modelli. Nel primo introduciamo, oltre a $educ$:

- $exper$ che rappresenta gli anni di esperienza nel mercato del lavoro.
  
Nel secondo oltre a $educ$ e $exper$ aggiungiamo:

- $tenure$ che rappresenta il numero di anni presso l'attuale datore di lavoro.

$$
wage_i = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + u_i
$$
$$
wage_i = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + \beta_3 tenure_i + u_i
$$

```{r}
# Regressione
library(fixest)
reg_wage1 <- feols(wage ~ educ, data = wage1, vcov = "hetero")
reg_wage2 <- feols(wage ~ educ + exper, data = wage1, vcov = "hetero")
reg_wage3 <- feols(wage ~ educ + exper + tenure, data = wage1, vcov = "hetero")
```

```{r}
#| label: tab1
#| tbl-cap: Risultati stima OLS. Variabile dipendente wage
#| results: asis
modelsummary(list("Wage" = reg_wage1, "Wage" = reg_wage2, "Wage" = reg_wage3), output = "markdown", gof_omit = "AIC|BIC|RMSE")
```

**Interpretazione**:

Nell’interpretazione dei coefficienti consideriamo l’effetto di una variabile tenendo costanti tutte le altre.

Siamo ancora interessati all'effetto di $educ$ su $wage$, tenendo fissi gli altri fattori. Togliamo $exper$ dal termine di errore e lo consideriamo esplicitamente nel modello.

Un anno in più di istruzione è associato ad un aumento, in media, del salario orario di 0.64\$ tenendo costante gli anni di esperienza nel mercato del lavoro.

In alternativa potremmo essere interessati all'effetto dell'esperienza sul salario orario mantenendo costanti gli effetti degli anni di istruzione. E quindi: un anno aggiuntivo di esperienza nel mercato del lavoro ($exper$) è associato ad un aumento, in media, del salario orario di 0.070 \$ tenendo costante gli anni di istruzione.

In questo caso l'assunzione chiave (zero conditional mean) è $E(u_i | educ_i, exper_i) = 0$

**Il terzo modello**:

Un anno in più di istruzione è associato ad un aumento, in media, del salario orario di 0.59\$ mantenendo costante gli anni di esperienza nel mercato del lavoro e gli anni con lo stesso datore di lavoro.

L'assunzione chiave (zero conditional mean) è $E(u_i | educ_i, exper_i, tenure_i) = 0$

#### Aggiungendo una dummy:

```{r}
# Regressione
reg_wage4 <- feols(wage ~ educ + exper + tenure + female, data = wage1, vcov = "hetero")
modelsummary(list("Wage" = reg_wage3, "Wage" = reg_wage4), output = "markdown", gof_omit = "AIC|BIC|RMSE")
```

In questa regressione stiamo “controllando” anche per il genere.

Se $E(u_i | educ_i, exper_i, tenure_i, female_i) = 0$ allora $\beta_1$ rappresenta l’effetto causale di un anno in più di istruzione sul salario orario, **a parità** di esperienza, $tenure$ e genere.


#### Cambio unità di misura:

Se $wage$ fosse espresso su base mensile: lavoratori a tempo pieno, 7 ore al giorno, 5 giorni a settimana, 4 settimane al mese. Cioè 7x5x4 = 140

Creiamo una nuova variabile wage_monthly = $wage$ x 140

```{r}
wage1 <- wage1 %>%
  mutate(wage_monthly = wage*140)


reg_wage_dummy3 <- feols(wage_monthly ~ educ + exper + tenure + female, data = wage1, vcov = "hetero")
modelsummary(list("Wage" = reg_wage4, "Wage Monthly" = reg_wage_dummy3), output = "markdown", gof_omit = "AIC|BIC|RMSE") 
```

Un anno in più di istruzione è associato, in media, a un aumento del salario orario 0.572$ in più, a parità di genere, esperienza e $tenure$


Vedremo in futuro poi cosa cambia considerando l'interazione $educ$ * $female$.

E se $educ$ fosse misurato in mesi di istruzione? 

In [Esercitazione 1](https://andrerecio.github.io/econometria-triennale/Esercitazione1.html) abbiamo visto che nel modello di regressione semplice, esprimendo $educ$ in mesi moltiplicando per 12, l'intercetta non cambiava e il coefficiente associato a $educ$ era diviso 12.

Il coefficente associato ad $educ montlhy$ sarà pari a 0.048 \$ (0.572/12). Un mese in più di istruzione è associato, in media, a un aumento del salario orario 0.048$ in più, a parità di genere, esperienza e $tenure$.
Il resto dei coefficienti, inclusa l'intercetta e la dummy, rimangono invariati. 


## Wage e genere:

In [Esercitazione 1](https://andrerecio.github.io/econometria-triennale/Esercitazione1.html) avevamo stimato la seguente regressione:

$$
wage_i = \beta_0 + \beta_1 female_i + u_i,
$$

```{r}
reg_wage_dummy <- feols(wage ~ female, data = wage1, vcov = "hetero")
reg_wage_dummy2 <- feols(wage ~ female + educ + exper + tenure, data = wage1, vcov = "hetero")
modelsummary(list("Wage (Esercitazione 1)" = reg_wage_dummy, "Wage" = reg_wage_dummy2), output = "markdown", gof_omit = "AIC|BIC|RMSE")
```

I risultati della prima regressione mostrano che il salario medio degli uomini (rappresentato dall'intercetta) è di circa 7.1 dollari (del 1976) per ora.

Il coefficiente della variabile dummy per il genere femminile (female) è -2.51 dollari. Le donne guadagnano in media circa 2.51 dollari in meno rispetto agli uomini. Il salario medio delle donne è di circa 4.58 dollari per ora (7.1 - 2.51).


Nella seconda regressione controlliamo per differenze nei livelli istruzione, esperienza, e $tenure$.


In questa seconda regressione l'intercetta (negativa) degli uomini non ha molto senso. Anche perhé nessun individuo ha zero di $educ$, $exper$ e $tenure$ nel campione.


Il coefficiente di $female$ rappresenta la differenza media nel salario orario tra un uomo e un donna che hanno lo stesso livello di istruzione, esperienza e $tenure$. Le donne guadagnano in media 1.81$ in meno rispetto agli uomini a parità di $educ$, $exper$ e $tenure$.


Siccome controllliamo per $educ$, $exper$, e $tenure$, il differenziale del salario orario di 1.81$ non può essere spiegato da diversi livelli medi di istruzione, esperienza, e tenure tra uomini e donne. 

Il differenziale di 1,81 dollari è dovuto al genere o a fattori associati al genere che non abbiamo controllato nella regressione.


## College GPA:

`gpa1` contiene la media dei voti durante il college ($colGPA$), scuola superiore GPA ($hsGPA$) e l'achievement test score ($ACT$) di 141 studenti di grandi università. 
$colGPA$ e $hsGPA$ sono in scala di quattro punti.

$ACT$ (American College Testing) è un test standardizzato diffuso negli USA per l'ammissione al College. Composto da 4 sezioni va da una scala da 1 a 36.

```{r}
data("gpa1", package = "wooldridge")
```

```{r}
head(gpa1)
```

```{r}
# Statistiche descrittive
statgpa <- gpa1 %>%
       summarise(
       colgpa_mean = mean(colGPA, na.rm = TRUE),
       hsgpa_mean = mean(hsGPA, na.rm = TRUE),
       act_mean = mean(ACT, na.rm = TRUE),
  )

# Visualizzazione dei risultati
statgpa %>%
  kable(caption = "Statistiche descrittive",
        digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

```{r}
summary(gpa1$colGPA)
summary(gpa1$hsGPA)
```

**Regressione**:

$$
colGPA = \beta_0 + \beta_1 hsGPA_i + \beta_2 ACT_i + u_i
$$


```{r}
reg_gpa1 <- feols(colGPA ~ hsGPA + ACT, data = gpa1, vcov = "hetero")
modelsummary(list("colGPA" = reg_gpa1), output = "markdown", gof_omit = "AIC|BIC|RMSE")
```

A parità di $ACT$, un punto in più di high school GPA è associato, in media, a 0.453 punti in più di $colGPA$.


**Nota**: il valore massimo di $hsGPA$ è 4.


$R^2$: $hsGPA$ e $ACT$ insieme spiegano il 17.6 \% della variazione di $colGPA$.


Cambiamo la scala di $hsGPA$
```{r}
gpa1 <- gpa1 %>%
  mutate(hsGPA100 = hsGPA *25)
```

```{r}
reg_gpa2 <- feols(colGPA ~ hsGPA100 + ACT, data = gpa1, vcov = "hetero")
modelsummary(list("colGPA100" = reg_gpa2), output = "markdown", gof_omit = "AIC|BIC|RMSE")
```

Un aumento di 1 punto (su scala fino a 100) di $hsGPA$ è associato, in media, a un aumento di 0.0181 punti in $colGPA$.

Come prima 0.0181 = 0.453/25

Aggiungiamo come regressore $skipped$ che rappresenta la media di lezioni perse (saltate) a settimana:

$$
colGPA = \beta_0 + \beta_1 hsGPA_i + \beta_2 ACT_i + \beta_3 skipped_i + u_i
$$


```{r}
reg_gpa3 <- feols(colGPA ~ hsGPA + ACT + skipped, data = gpa1, vcov = "hetero")
modelsummary(list("colGPA" = reg_gpa1, "colGPA" = reg_gpa3), output ="markdown", gof_omit = "AIC|BIC|RMSE")
```

Un aumento di 1 punto di $hsGPA$ è associato, in media, a un aumento di 0.412 punti in $colGPA$ a parità di $ACT$ e $skipped$.

E se fossimo interessati altri due coefficienti?

- Un aumento di 1 punto in $ACT$, a parità di $hsGPA$ e $skipped$, è associato a un aumento, in media, di 0.015 punti in $colGPA$. t-statistic è 1.36, cosa significa? 

- A parità di $hsGPA$ e $ACT$, una lezione persa (saltata) in più a settimana è associata a un calo, in media, di 0.083 punti in $colGPA$.

### Effetti del PC su college GPA

Regrediamo $colGPA$ su una variabile dummy $PC$ che è uguale ad uno se lo studente ha un personal computer.

Stimiamo le seguenti regressioni:

- `reg_gpa` regressione lineare semplice
- `reg_gpa_pc` regressione con $hsGPA$ e $ACT$
- `reg_gpa_pc3` regressione di $hsGPA$ su $ACT$, $mothcoll$ (= 1 se la madre ha almeno una laurea triennale, BA) e $fathcoll$ (= 1 se il padre ha almeno una laurea triennale, BA).

```{r}
reg_gpa_pc1 <- feols(colGPA ~ PC, data = gpa1, vcov = "hetero")
reg_gpa_pc2 <- feols(colGPA ~ PC + hsGPA + ACT , data = gpa1, vcov = "hetero")
reg_gpa_pc3 <- feols(colGPA ~ PC + hsGPA + ACT + mothcoll + fathcoll , data = gpa1, vcov = "hetero")
modelsummary(list("colGPA" = reg_gpa_pc1, "colGPA" = reg_gpa_pc2, "colGPA" = reg_gpa_pc3), output = "markdown", gof_omit = "AIC|BIC|RMSE")

```

- Prima regressione: Uno studente che possiede un PC ha, in media, un $colGPA$ più alto di circa 0.17 punti rispetto a uno studente che non lo possiede

- Seconda regressione: A parità di $hsGPA$ e di punteggio $ACT$, uno studente che possiede un PC ha, in media, $colGPA$ più alto di circa 0.16 punti rispetto a uno studente che non lo possiede.

- Terza regressione:  A parità di $hsGPA$, punteggio $ACT$ e istruzione dei genitori ($mothcoll$ e $fathcoll$), uno studente che possiede un PC ha, in media, $colGPA$ più alto di circa 0.15 punti rispetto a uno studente che non lo possiede.

## Dummy con categorie multiple:


Stimiamo un modello che tiene conto delle differenze salariali tra quattro gruppi: uomini sposati, donne sposate, uomini single e donne single. 

Per interpretare i coefficienti delle variabili dummy, dobbiamo scegliare un gruppo di riferimento.

Dobbiamo quindi definire delle dummy per gli altri tre gruppi.

**Variabili dummy**:
Siamo interessati a uomini single (è la nostra categoria omessa). Creiamo le seguenti variabili:

- $marrmale$ uguale ad 1 se uomo e sposato
- $marrfemale$ uguale ad 1 se donna e sposata
- $singfem$ uguale ad 1 se donna e single


Se includessimo anche $female$?
Collinearità perfetta (trappola delle variabili dummy).

```{r}
wage1 <- wage1 %>%
  mutate(
    marrmale = ifelse(female == 0 & married == 1, 1, 0),
    marrfemale = ifelse(female == 1 & married == 1, 1, 0),
    singfem = ifelse(female == 1 & married == 0, 1, 0)
  )
```

Nel caso in cui è necessario creare diversi dummy per Regione (ad esempio) è utile usare la libreria `library(fastDummies)`

```{r}
reg_wage_sm1 <- feols(wage ~ marrmale + marrfemale + singfem, data = wage1, vcov = "hetero")
reg_wage_sm2 <- feols(wage ~ marrmale + marrfemale + singfem + educ + exper + tenure, data = wage1, vcov = "hetero")
modelsummary(list("Wage" = reg_wage_sm1, "Wage" = reg_wage_sm2), output = "markdown", gof_omit = "AIC|BIC|RMSE")
```

**Interpretazione dei coefficienti (seconda colonna):**

- In media, un uomo sposato, guadagna 1.822 dollari (per ora) in più rispetto ad un uomo single a parità di istruzione, esperienza e anni con l'attuale datore di lavoro ($tenure$).

- In media, una donna sposata, guadagna 0.882 dollari (per ora) in meno rispetto ad un uomo single a parità di istruzione, esperienza e $tenure$.

- In media, una donna single, guadagna 0.309 dollari (per ora) in meno rispetto ad un uomo single a parità di istruzione, esperienza e $tenure$.

**Attenzione!**: sono statisticamente significativi al 5%? 



**Trappola delle variabili dummy:**

Creiamo (come in Esercitazione 1) la variabile $male$.
```{r}
wage1 <- wage1 %>% 
  mutate(male = 1 - female)
```

Regressione di $wage$ su $marrmale$, $marrfemale$, $singfem$, $female$ e $male$

```{r}
reg_wage_sm3 <- feols(wage ~ marrmale + marrfemale + singfem + female + male, data = wage1, vcov = "hetero")
reg_wage_sm3
```

$female$ e $male$ non sono necessarie.

Perché:

- $female$ = $marrfemale$ + $singfem$

- $male$ = $marrmale$ + $singmale$ (il nostro gruppo di riferimento per come abbiamo costruito le dummy)

R rimuove automaticamente le due variabili perfettamente collineari.
